{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fresh start on experiments in stackelberg actor critic. Only write down what is sure to be right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import ENV_NAMES\n",
    "\n",
    "from models.critic import Critic, PixelCritic\n",
    "from models.discrete_actor import DiscreteActor, DiscretePixelActor\n",
    "import models.params\n",
    "from models.params import DynParam\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import functools\n",
    "import gymnax\n",
    "import jax\n",
    "from jax import grad, jacfwd, jacrev\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from jax import flatten_util\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "import json\n",
    "\n",
    "\n",
    "from algos.core.env_config import Hyperparams\n",
    "# from algos.core.env_config import ENV_CONFIG\n",
    "from algos.StackelbergRL.understanding_gradients import cosine_similarity, project_B_onto_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one rollout and transitions and last observation for advantage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flax.struct.dataclass\n",
    "class Transition:\n",
    "    \"\"\"A data class that stores a state transition.\"\"\"\n",
    "    observation: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    done: jnp.ndarray\n",
    "\n",
    "def run_rollout(env, env_params, length, actor_state, rng_key):\n",
    "    \"\"\"Collects an actor policy rollout with a fixed number of steps.\"\"\"\n",
    "    rng_key, reset_key = jax.random.split(rng_key, 2)\n",
    "    observation, env_state = env.reset(reset_key, env_params)\n",
    "\n",
    "    def step(rollout_state, x):\n",
    "        \"\"\"Advances the environment by 1 step by sampling from the policy.\"\"\"\n",
    "        # Sample action\n",
    "        actor_state, env_state, observation, rng_key = rollout_state\n",
    "        rng_key, action_key, step_key = jax.random.split(rng_key, 3)\n",
    "        action_dist = actor_state.apply_fn(actor_state.params, observation)\n",
    "        action = action_dist.sample(seed=action_key)\n",
    "\n",
    "        # Run environment step\n",
    "        next_observation, next_state, reward, done, i = env.step(\n",
    "            step_key, env_state, action, env_params,\n",
    "        )\n",
    "        transition = Transition(\n",
    "            observation, action, reward, done,\n",
    "        )\n",
    "\n",
    "        next_step = (actor_state, next_state, next_observation, rng_key)\n",
    "        return (next_step, transition)\n",
    "\n",
    "    rollout_state, transitions = jax.lax.scan(\n",
    "        step,\n",
    "        init=(actor_state, env_state, observation, rng_key),\n",
    "        length=length,\n",
    "    )\n",
    "    a, n, last_observation, r = rollout_state\n",
    "    return (transitions, last_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the advantages using GAE and the targets using Monte Carlo as indicated by the ratliff paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def calc_values(critic_state, transitions, last_observation, discount_rate, gae_lambda = 0.97):\n",
    "    \"\"\"Calculates the advantage estimate at each time step.\"\"\"\n",
    "    values = jax.vmap(critic_state.apply_fn, in_axes=(None, 0))(critic_state.params, transitions.observation)\n",
    "    last_val = critic_state.apply_fn(critic_state.params, last_observation)\n",
    "\n",
    "    def _calculate_gae(values, last_val):\n",
    "        def _get_advantages(gae_and_next_value, transition):\n",
    "            gae, next_value = gae_and_next_value\n",
    "            done, value, reward = transition\n",
    "            delta = reward + discount_rate * next_value * (1 - done) - value\n",
    "            gae = (\n",
    "                delta\n",
    "                + discount_rate * gae_lambda * (1 - done) * gae\n",
    "            )\n",
    "            return (gae, value), gae\n",
    "\n",
    "        _, advantages = jax.lax.scan(\n",
    "            _get_advantages,\n",
    "            init = (jnp.zeros_like(last_val), last_val),\n",
    "            xs = (transitions.done, values, transitions.reward),\n",
    "            reverse=True,\n",
    "            unroll=16,\n",
    "        )\n",
    "\n",
    "        # def _get_targets(cummulative_reward, transition):\n",
    "        #     reward, done = transition\n",
    "        #     cum_rew = reward + discount_rate * cummulative_reward * (1 - done)\n",
    "        #     return cum_rew, cum_rew\n",
    "\n",
    "        # _, targets = jax.lax.scan(\n",
    "        #     _get_targets, \n",
    "        #     init = last_val,\n",
    "        #     xs = (transitions.reward, transitions.done),\n",
    "        #     reverse=True,\n",
    "        # )\n",
    "\n",
    "        return advantages, advantages + values\n",
    "\n",
    "    advantages, targets = _calculate_gae(values, last_val)\n",
    "\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    return (advantages, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update functions for both critic and actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# follower-objective\n",
    "def target_loss(params, transitions, targets, critic_state):\n",
    "    \"\"\"Calculates the mean squared error on a batch of transitions.\"\"\"\n",
    "    values = jax.vmap(critic_state.apply_fn, in_axes=(None, 0))(params, transitions.observation)\n",
    "    errors = jnp.square(targets - values)\n",
    "    # jax.debug.print(f\"targets shape: {targets.shape()} values shape: {values.shape()}\")\n",
    "    return jnp.mean(errors)\n",
    "\n",
    "def update_leaderactor(actor_state, critic_state, transitions, advantages, targets, vanilla=False, lambda_reg=0):\n",
    "    # Define the loss functions\n",
    "    def advantage_loss(params, transitions, advantages, lambda_reg = 0.01):\n",
    "        action_dists = jax.vmap(actor_state.apply_fn, in_axes=(None, 0))(params, transitions.observation)\n",
    "        log_probs = action_dists.log_prob(transitions.action)\n",
    "\n",
    "        # l2_loss = lambda_reg * sum(jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "        return -jnp.mean(advantages * log_probs) \n",
    "    \n",
    "    def leader_f2_loss(actor_params, critic_params, transitions, targets):\n",
    "        action_dists = jax.vmap(actor_state.apply_fn, in_axes=(None, 0))(actor_params, transitions.observation)\n",
    "        log_probs = action_dists.log_prob(transitions.action)\n",
    "        values = jax.vmap(critic_state.apply_fn, in_axes=(None, 0))(critic_params, transitions.observation)\n",
    "\n",
    "        # l2_loss = lambda_reg * sum(jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(critic_params))\n",
    "        \n",
    "        return 2 * jnp.mean(log_probs * advantages) * (targets[0] - values[0])\n",
    "    \n",
    "    # Single Gradients\n",
    "    vgj = jax.value_and_grad(advantage_loss)\n",
    "    actor_loss, grad_theta_J = vgj(actor_state.params, transitions, advantages)\n",
    "    grad_w_J = grad(target_loss, 0)(critic_state.params, transitions, targets, critic_state)\n",
    "\n",
    "    def hvp(v):\n",
    "        critic_params_flat, unravel_fn = jax.flatten_util.ravel_pytree(critic_state.params)\n",
    "        def loss_grad_flat(p):\n",
    "            return jax.flatten_util.ravel_pytree(\n",
    "                jax.grad(target_loss, argnums=0)(unravel_fn(p), transitions, targets, critic_state)\n",
    "            )[0]\n",
    "        hvp = jax.jvp(loss_grad_flat, (critic_params_flat,), (v,))[1] + lambda_reg * v\n",
    "        return hvp\n",
    "    \n",
    "    grad_w_J_flat, unflatten_fn = jax.flatten_util.ravel_pytree(grad_w_J)\n",
    "    def cg_solve(v):\n",
    "        return jax.scipy.sparse.linalg.cg(hvp, v, maxiter=10, tol=1e-10)[0]\n",
    "    inverse_hvp_flat = cg_solve(grad_w_J_flat)\n",
    "    inverse_hvp = unflatten_fn(inverse_hvp_flat)\n",
    "\n",
    "    # 6. Compute mixed gradient and its transpose: [∇²_θ,ν V_s(ν, θ*(ν))]^T\n",
    "    def mixed_grad_fn(policy_params, critic_params):\n",
    "        return jax.grad(leader_f2_loss)(policy_params, critic_params, transitions, targets)\n",
    "\n",
    "    # 7. Compute the final product: [∇²_θ,ν V_s(ν, θ*(ν))]^T * [∇²_θ V_s(ν, θ*(ν))]^(-1) * ∇_θ L_pref(ν)\n",
    "    # We use JVP to compute this product efficiently\n",
    "    _, final_product = jax.jvp(\n",
    "        lambda p: mixed_grad_fn(actor_state.params, p),\n",
    "        (critic_state.params,),\n",
    "        (inverse_hvp,)\n",
    "    )\n",
    "\n",
    "    # vanilla=True\n",
    "\n",
    "    # final_product = clip_grad_norm(final_product, 0.2*optax.global_norm(grad_theta_J))\n",
    "    hypergradient = jax.tree_util.tree_map(lambda x, y: x - y, grad_theta_J, final_product)\n",
    "    hypergradient = jax.lax.cond(vanilla, lambda: grad_theta_J, lambda: hypergradient)\n",
    "    actor_state = actor_state.apply_gradients(grads=hypergradient)\n",
    "    \n",
    "    #print all norms\n",
    "    hypergradient_norms = optax.global_norm(hypergradient)\n",
    "    final_product_norms = optax.global_norm(final_product)\n",
    "    co_sim = cosine_similarity(final_product, grad_theta_J)\n",
    "\n",
    "    return (actor_state, (hypergradient_norms, final_product_norms, co_sim), actor_loss)\n",
    "\n",
    "def clip_grad_norm(grad, max_norm):\n",
    "    norm = optax.global_norm(grad)\n",
    "    factor = jnp.minimum(max_norm, max_norm / (norm + 1e-6))\n",
    "    return jax.tree_map((lambda x: x * factor), grad)\n",
    "\n",
    "def update_critic(critic_state, transitions, targets):\n",
    "    \"\"\"Calculates and applies the value target gradient at each time step.\"\"\"\n",
    "    target_grad = jax.value_and_grad(target_loss)\n",
    "    loss, grads = target_grad(critic_state.params, transitions, targets, critic_state)\n",
    "    critic_state = critic_state.apply_gradients(grads=grads)\n",
    "    return (critic_state, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update functions to perform update on a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_episode_rewards(transitions):\n",
    "    \"\"\"Calculates the total real reward for each episode.\"\"\"\n",
    "    def calc_reward(prev_total, transition):\n",
    "        \"\"\"Adds the current reward to the total.\"\"\"\n",
    "        total = prev_total + transition.reward\n",
    "        next_total = total * (1 - transition.done)\n",
    "        return (next_total, total)\n",
    "\n",
    "    s, rewards = jax.lax.scan(\n",
    "        calc_reward,\n",
    "        init=jnp.float32(0),\n",
    "        xs=transitions,\n",
    "    )\n",
    "    return rewards\n",
    "\n",
    "def run_update(env, env_params, actor_state, critic_state, rng_key, hyperparams, vanilla=False, lam=0):\n",
    "    \"\"\"Runs an iteration of the training loop with the vanilla parallel update.\"\"\"\n",
    "    rng_key, rollout_key = jax.random.split(rng_key, 2)\n",
    "    transitions, last_observation = run_rollout(env, env_params, hyperparams.rollout_len, actor_state, rollout_key)\n",
    "    advantages, targets = calc_values(critic_state, transitions, last_observation, hyperparams.discount_rate)\n",
    "\n",
    "    actor_state, actor_info, actor_loss = update_leaderactor(actor_state, critic_state, transitions, advantages, targets, vanilla, lam)\n",
    "    critic_loss = 0\n",
    "    for c in range(hyperparams[\"nested_updates\"]):\n",
    "        critic_state, critic_loss = update_critic(critic_state, transitions, targets)\n",
    "\n",
    "    total_rewards = calc_episode_rewards(transitions)\n",
    "    average_reward = jnp.sum(total_rewards * transitions.done) / jnp.sum(transitions.done)\n",
    "    return (actor_state, critic_state, (average_reward, actor_info, actor_loss, critic_loss), rng_key)\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=0)\n",
    "def run_batch(env, env_params, actor_state, critic_state, rng_key, hyperparams, vanilla=False, lam=0):\n",
    "    \"\"\"Trains the model for a batch of updates.\"\"\"\n",
    "    def run_once(batch_state, x):\n",
    "        \"\"\"Runs an update and carries over the train state.\"\"\"\n",
    "        actor_state, critic_state, rng_key = batch_state\n",
    "        actor_state, critic_state, metrics, rng_key = \\\n",
    "            run_update(env, env_params, actor_state, critic_state, rng_key, hyperparams, vanilla, lam)\n",
    "        return ((actor_state, critic_state, rng_key), metrics)\n",
    "\n",
    "    batch_state, batch_metrics = jax.lax.scan(\n",
    "        run_once,\n",
    "        init=(actor_state, critic_state, rng_key),\n",
    "        length=hyperparams.batch_count,\n",
    "    )\n",
    "    actor_state, critic_state, rng_key = batch_state\n",
    "    return (actor_state, critic_state, batch_metrics, rng_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_CONFIG = {\n",
    "    \"cartpole\": {\n",
    "        \"actor_model\": DiscreteActor,\n",
    "        \"actor_params\": [(64, 2), DynParam.ActionCount],  # Two hidden layers with 64 units each\n",
    "        \"critic_model\": Critic,\n",
    "        \"critic_params\": [(64, 2)],\n",
    "        \"hyperparams\": Hyperparams(\n",
    "            num_updates=500,\n",
    "            batch_count=25,\n",
    "            rollout_len=2000,\n",
    "            discount_rate=0.99,\n",
    "            actor_learning_rate=0.0025,\n",
    "            nested_updates=25,\n",
    "            critic_learning_rate=0.008,\n",
    "            adam_eps=1e-5,\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "def train(env_key, seed, logger, verbose = False, metrics=None, vanilla=False, save_charts=False, description=None, lam=0):\n",
    "    # Create environment\n",
    "    config = ENV_CONFIG[env_key]\n",
    "    hyperparams = config[\"hyperparams\"]\n",
    "\n",
    "    rng_key, actor_key, critic_key = jax.random.split(jax.random.key(seed), 3)\n",
    "    env, env_params = gymnax.make(ENV_NAMES[env_key])\n",
    "    empty_observation = jnp.empty(env.observation_space(env_params).shape)\n",
    "\n",
    "    # Initialize actor model\n",
    "    actor_model_params = models.params.init(env, env_params, config[\"actor_params\"])\n",
    "    actor = config[\"actor_model\"](*actor_model_params)\n",
    "    actor_params = actor.init(actor_key, empty_observation)\n",
    "\n",
    "    # Initialize critic model\n",
    "    critic_model_params = models.params.init(env, env_params, config[\"critic_params\"])\n",
    "    critic = config[\"critic_model\"](*critic_model_params)\n",
    "    critic_params = critic.init(critic_key, empty_observation)\n",
    "\n",
    "    # Create actor and critic train states\n",
    "    actor_state = TrainState.create(\n",
    "        apply_fn=jax.jit(actor.apply),\n",
    "        params=actor_params,\n",
    "        tx=optax.adam(hyperparams[\"actor_learning_rate\"], eps=hyperparams.adam_eps),\n",
    "    )\n",
    "    critic_state = TrainState.create(\n",
    "        apply_fn=jax.jit(critic.apply),\n",
    "        params=critic_params,\n",
    "        tx=optax.adam(hyperparams[\"critic_learning_rate\"], eps=hyperparams.adam_eps),\n",
    "    )\n",
    "    \n",
    "    # Set logger info\n",
    "    logger.set_interval(hyperparams.rollout_len)\n",
    "\n",
    "    # Run the training loop\n",
    "    num_batches = int(hyperparams.num_updates / hyperparams.batch_count)\n",
    "    for b in range(num_batches):\n",
    "        actor_state, critic_state, batch_metrics, rng_key = \\\n",
    "            run_batch(env, env_params, actor_state, critic_state, rng_key, hyperparams, vanilla, lam)\n",
    "        logger.log_metrics({\n",
    "            \"reward\": batch_metrics[0],\n",
    "            \"actor_loss\": batch_metrics[2],\n",
    "            \"critic_loss\": batch_metrics[3],\n",
    "            \"hypergradient\": batch_metrics[1][0],\n",
    "            \"final_product\": batch_metrics[1][1],\n",
    "            \"cosine_similarities\": batch_metrics[1][2]\n",
    "        })\n",
    "        if verbose:\n",
    "            print(f\"[Update {(b + 1) * hyperparams.batch_count}]: Average reward {batch_metrics[0][-1]}, Hypergradient Norm {jnp.mean(batch_metrics[1][0])}, finalProduct Norm{jnp.mean(batch_metrics[1][1])}, cosine similarity {batch_metrics[1][2][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import jax\n",
    "import os\n",
    "\n",
    "from algos.StackelbergRL import ratliff, stac_critic, stac_Actor_newGrad, stac_Critic\n",
    "from algos.baselines import discrete_actor_critic, discrete_ppo, discrete_reinforce, actor_critic_NoNesting\n",
    "from bilevel_actor_critic import unrolling_actor_redo, lambda_regret\n",
    "\n",
    "from loggers.chart_logger import ChartLogger\n",
    "from algos.core.config import ALGO_CONFIG\n",
    "\n",
    "algo = \"cartpole\"\n",
    "\n",
    "metrics = [\n",
    "    \"reward\",\n",
    "    \"actor_loss\",\n",
    "    \"critic_loss\",\n",
    "    \"hypergradient\",\n",
    "    \"final_product\",\n",
    "    \"cosine_similarities\"\n",
    "] \n",
    "logger = ChartLogger(metrics)\n",
    "\n",
    "config = ALGO_CONFIG[algo]\n",
    "description = config[\"description\"]\n",
    "\n",
    "if not(args.description==\"\"):\n",
    "        description = args.description\n",
    "\n",
    "folder_path = f\"charts/{args.algo}/{args.task}_{description}\"\n",
    "for metric in metrics:\n",
    "        file_path = f\"{folder_path}/{args.task}_{metric}.png\"\n",
    "        \n",
    "        logger.set_info(\n",
    "            metric,\n",
    "            f\"[{args.task}] SA2C {metric}\",\n",
    "            file_path,\n",
    "        )\n",
    "\n",
    "algo = algos[args.algo]\n",
    "# Ensure the data directory for the task exists\n",
    "os.makedirs(f'data/{args.task}', exist_ok=True)\n",
    "if not(vanilla):\n",
    "    # print(\"yes!\")\n",
    "    algo.train(args.task, 0, logger, verbose=True, lam=args.lam)\n",
    "else:\n",
    "    algo.train(args.task, 0, logger, verbose=True)\n",
    "logger.log_to_csv(f'data/{args.task}/{args.algo}_{description}.csv')\n",
    "\n",
    "# Plot metrics\n",
    "if(args.plot):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    for m in metrics:\n",
    "        logger.plot_metric(m)\n",
    "\n",
    "train(\"cartpole\", 0, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
