method: bayes
metric:
  goal: maximize
  name: Reward
parameters:
  ACTIVATION:
    distribution: categorical
    values:
      - tanh
  ANNEAL_LR:
    distribution: categorical
    values:
      - "false"
  CLIP_EPS:
    distribution: uniform
    values:
     - 0.2
  DEBUG:
    distribution: categorical
    values:
      - "true"
  ENT_COEF:
    distribution: uniform
    values:
     - 0.0
  ENV_NAME:
    distribution: categorical
    values:
      - hopper
  GAE_LAMBDA:
    distribution: uniform
    values:
      - 0.95

  GAMMA:
    distribution: uniform
    values:
     - 0.99

  # sweep over IHVP_BOUND:
  IHVP_BOUND:
    distribution: uniform
    max: 2.0
    min: 1.0
  LR:
    distribution: uniform
    values:
      - 0.0003
  NORMALIZE_ENV:
    distribution: categorical
    values:
      - "true"
  NUM_ENVS:
    distribution: int_uniform
    values:
     - 32
  NUM_MINIBATCHES:
    distribution: int_uniform
    values:
     - 32
  NUM_STEPS:
    distribution: int_uniform
    values: 
     - 640
  # NUM_UPDATES:
  #   distribution: int_uniform
  #   max: 4882
  #   min: 31
  TOTAL_TIMESTEPS:
    distribution: int_uniform
    values: 
    - 20000000
  UPDATE_EPOCHS:
    distribution: int_uniform
    max: 20
    min: 2
  VF_COEF:
    distribution: uniform
    max: 1
    min: 0.25
  actor-LR:
    distribution: uniform
    value:
     - 0.0003
  critic-LR:
    distribution: uniform
    max: 0.002
    min: 0.0005
  nested_updates:
    distribution: int_uniform
    max: 20
    min: 2
  nystrom_rank:
    distribution: int_uniform
    max: 20
    min: 5
  nystrom_rho:
    distribution: int_uniform
    max: 100
    min: 25
  vanilla:
    distribution: categorical
    values:
      - "true"
      - "false"
program: train.py
